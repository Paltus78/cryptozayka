"""Project evaluation strategy ‚Äî senior edition (OpenAIError-safe, legacy alias)."""
from __future__ import annotations

import asyncio
import json
import logging
import os
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any

# ---------------------------------------------------------------------------
# OpenAI import ‚Äî —Å–æ–≤–º–µ—Å—Ç–∏–º —Å–æ –≤—Å–µ–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ SDK
# ---------------------------------------------------------------------------
try:  # OpenAI ‚â• 1.0
    from openai import AsyncOpenAI, OpenAIError                # type: ignore
except ImportError:  # OpenAI < 1.0
    from openai import AsyncOpenAI                              # type: ignore
    from openai.error import OpenAIError                        # type: ignore

from ..settings import get_settings

# ---------------------------------------------------------------------------
# Config & constants
# ---------------------------------------------------------------------------

settings = get_settings()
log = logging.getLogger(__name__)

MODEL = "gpt-4-0125-preview"

_prompts_dir = os.getenv("PROMPTS_DIR")
if _prompts_dir:
    PROMPT_FILE = Path(_prompts_dir) / "project_eval.md"
else:
    PROMPT_FILE = Path(__file__).resolve().parents[2] / "prompts" / "project_eval.md"

MAX_DESC_LEN = 512        # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ ‚Üí –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
RETRY_ATTEMPTS = 3
RETRY_DELAY = 2           # —Å–µ–∫—É–Ω–¥, —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ

# ---------------------------------------------------------------------------
# Data types
# ---------------------------------------------------------------------------

class Verdict(str, Enum):
    GREEN = "green"
    YELLOW = "yellow"
    RED   = "red"
    ERROR = "error"

@dataclass(slots=True)
class EvaluationResult:
    project: str
    verdict: Verdict
    explanation: str
    raw_model_answer: str
    model: str = MODEL
    tokens: int | None = None     # usage.total_tokens

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ legacy alias, —á—Ç–æ–±—ã —Å—Ç–∞—Ä—ã–π –∫–æ–¥ (api.py, executor.py) –Ω–µ –ø–∞–¥–∞–ª ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class AnalysisResult(EvaluationResult):   # noqa: N801
    """Back-compat shim. –£–¥–∞–ª–∏—Ç—å, –∫–æ–≥–¥–∞ –≤—Å–µ –∏–º–ø–æ—Ä—Ç—ã –æ–±–Ω–æ–≤–∏–º."""
    pass
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# GPT helper
# ---------------------------------------------------------------------------

client = AsyncOpenAI(api_key=settings.openai_api_key)

def _build_prompt(project: dict[str, Any]) -> str:
    if not PROMPT_FILE.exists():
        raise FileNotFoundError(f"Prompt file not found: {PROMPT_FILE}")
    tpl = PROMPT_FILE.read_text(encoding="utf-8")
    pj  = json.dumps(project, ensure_ascii=False, indent=2)
    return tpl.replace("{{ project_json }}", pj)

async def _call_gpt(prompt: str) -> tuple[str, int | None]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ GPT-4 —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –±—ç–∫–æ—Ñ—Ñ–æ–º."""
    delay = RETRY_DELAY
    for attempt in range(1, RETRY_ATTEMPTS + 1):
        try:
            resp = await client.chat.completions.create(
                model=MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=300,
            )
            return resp.choices[0].message.content, getattr(resp.usage, "total_tokens", None)
        except OpenAIError as e:
            log.warning("GPT call failed (%d/%d): %s", attempt, RETRY_ATTEMPTS, e)
            if attempt == RETRY_ATTEMPTS:
                raise
            await asyncio.sleep(delay)
            delay *= 2

# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------

async def analyze_project(name: str, description: str) -> EvaluationResult:
    """–ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞. –°–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å –ø—Ä–µ–∂–Ω–∏–º –∫–æ–¥–æ–º."""
    description = (description or "")[:MAX_DESC_LEN]
    project = {"name": name, "description": description}

    prompt = _build_prompt(project)

    try:
        answer, tokens = await _call_gpt(prompt)
        parsed = json.loads(answer)

        verdict_raw = parsed.get("verdict", "").lower()
        explanation = parsed.get("explanation", "").strip()

        if verdict_raw not in {v.value for v in Verdict}:
            raise ValueError(f"Unexpected verdict '{verdict_raw}'")

        verdict = Verdict(verdict_raw)

    except Exception as exc:
        log.exception("‚ùå GPT evaluation failed: %s", exc)
        verdict = Verdict.ERROR
        explanation = f"Evaluation error: {exc}"
        answer = str(exc)
        tokens = None

    result = EvaluationResult(
        project=name,
        verdict=verdict,
        explanation=explanation,
        raw_model_answer=answer,
        tokens=tokens,
    )
    if tokens:
        log.info("üìù %s ‚Üí %s (%s tokens)", name, verdict.value, tokens)

    return result
